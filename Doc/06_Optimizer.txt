這個檔案紀錄06_Optimizer.py中所有細節:


1.隨機梯度下降法 (Stochastic Gradient Descent, SGD) 和
    一般的梯度下降法（通常稱為批量梯度下降法或全量梯度下降法）有甚麼不同?
Ans:

***全量梯度下降法（Batch Gradient Descent）
A. 優點：
    1. 更新步驟穩定：每次梯度計算基於完整的數據集，因此梯度更新較為平穩。
    2. 收斂性好：由於使用了完整數據，更新過程較為一致，能夠穩定收斂到一個最小值。
    3. 適合小型數據集：對於數據集可以完全加載到內存中的情況，計算過程較為直接和穩定。
B. 缺點：
    1. 計算開銷大：每次更新需要處理整個數據集，對內存和計算資源的需求較高。
    2. 更新頻率低：每次迭代需要遍歷整個數據集，更新次數較少，訓練過程可能較慢。
    3. 無法處理大數據集：當數據集過大時，內存需求可能過高，導致計算和存儲困難。

***隨機梯度下降法（SGD）
A.優點：
    1. 計算開銷低：每次迭代只處理單個樣本或小批量樣本，內存和計算需求相對較低。
    2. 更新頻繁：每次迭代都進行一次梯度更新，能夠更快地進行參數更新。
    3. 能跳出局部最小值：由於隨機性，能夠幫助跳出局部最小值，潛在地找到全局最小值。
    4. 適合大數據集：能夠處理過大無法完全加載到內存中的數據集。
B. 缺點：
    1. 更新步驟波動：由於每次梯度計算基於部分數據，梯度更新可能較為不穩定，收斂過程中可能會出現波動。
    2. 收斂速度慢：可能需要更多的迭代步數才能收斂到較好的解，訓練過程不如全量梯度下降穩定。
    3. 需要調整學習率：學習率的選擇可能對訓練效果有很大影響，可能需要進行更多的超參數調整。

2. SGD需要自己調整學習率，BGD則不用是嗎?
Ans:
    A.SGD的學習率常常需要動態調整，使用學習率衰減（learning rate decay）
    或自適應學習率優化器（如Adam、RMSprop）來幫助改善訓練效果。
    
    B.在實際應用中，BGD也可以使用學習率衰減或其他技術來調整學習率，
    但整體而言對學習率的敏感度較低。

3. tf.GradientTape()是什麼?
Ans:
    "GradientTape()" 是TensorFlow中一個用於自動微分的上下文管理器，可以捕捉計算過程並計算梯度。

4.第43行中zip是什麼?
Ans:
    "opt_sgd_momentum.apply_gradients(zip(grads, [var]))"，是把梯度應用到Optimizer中
    "zip(grads, [var])"創建了一個梯度和變數的配對列表
    ，以便 apply_gradients 能夠正確地將每個梯度應用到對應的變數上。

5.第58行中，"lambda"是什麼?
Ans:
    lambda: 是 Python 的一個關鍵字，用於創建匿名函數（即沒有名字的函數）。
    lambda 函數通常用於簡單的、短小的函數定義。
    當懶得想函數名時可以用。


***整個程式碼，單純應用SGD，Adam...etc這類的Optimizer，
並使用GradientTape()函式將每個過程拆解出來解析。